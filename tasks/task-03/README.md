# Лабораторная работа №3 (N-grams)

## Синопсис лекции

Подробное описание мер ассоциативной связанности можно изучить в работе 
[М.В. Хохловой “Экспериментальная проверка методов выделения коллокаций”](https://blogs.helsinki.fi/slavica-helsingiensia/files/2019/11/sh34-21.pdf).
Меры ассоциативной связанности (association measures) - меры, вычисляющие силу связи между элементами в составе коллокации (параметры: частота совместной встречаемости, частота слова в корпусе, размер корпуса, и др.). MI (Mutual Information) чувствительна к низкочастотным словам, а t-score полезна для нахождения высочастотных коллокаций.

![MI](MI.png)

где n — ключевое слово (node); c — коллокат (collocate);
f (n, c) — частота встречаемости ключевого слова n в паре с коллокатом c; 
f(n), f(c) — абсолютные (независимые) частоты ключевого слова n и слов c в 
корпусе (тексте); N — общее число словоупотреблений в корпусе (тексте), ngram - 
количество слов в n-gram (для триграмм = 3), f(u_i) - абсолютная частота i-й 
униграммы в n-gram.

![log-likelihood](log-likelihood.png)
где O_ij , E_ij - наблюдаемая и ожидаемая частоты, 
ngram - количество слов в n-gram (для триграмм = 3). 
Рекомендуется более подробно прочитать стр. 346 в работе 
[М.В. Хохловой “Экспериментальная проверка методов выделения коллокаций”](https://blogs.helsinki.fi/slavica-helsingiensia/files/2019/11/sh34-21.pdf).

![t-score](t-score.png)
где n — ключевое слово (node); c — коллокат (collocate); 
f(n, c) — частота встречаемости ключевого слова n в паре с коллокатом ; 
f(n), f(c) — абсолютные (независимые) частоты ключевого слова n и слов c в 
корпусе (тексте); N — общее число словоупотреблений в корпусе (тексте), 
ngram - количество слов в n-gram (для триграмм = 3), 
f(ui) - абсолютная частота i-й униграммы в n-gram.


## Задание

Необходимо выполнить подсчет 3-граммы и соответствующих мер ассоциации для выбранного корпуса.

1. На основе первого практического задания сформировать данные (список) для расчета n-грамм по словоформам / лексемам (леммами) (в зависимости от доставшегося варианта). 
2. Очистить полученные данные от знаков пунктуации. Можно использовать регулярное выражение: `[^\P{P}-]+`
3. Привести полученные данные к нижнему регистру.
4. Очистить полученные данные от стоп слов. Можно использовать nltk.corpus stopwords.
5. Посчитать 3-граммы и их частоту.
6. Реализовать алгоритм расчета меры ассоциации (в зависимости от доставшегося варианта)
    * словоформы, MI
    * словоформы, t-score
    * словоформы, log-likelihood *
    * лексемы, MI
    * лексемы, t-score
    * лексемы, log-likelihood *
    

   `*` повышенный коэффициент

7. Проверить результаты  с помощью библиотеки NLTK, пример скрипта (без удаления стоп слов и пунктуации):
```
import nltk
from nltk.collocations import *
from nltk.corpus import PlaintextCorpusReader

bigram_measures = nltk.collocations.BigramAssocMeasures()
trigram_measures = nltk.collocations.TrigramAssocMeasures()

f = open('text.txt')
raw = f.read()

tokens = nltk.word_tokenize(raw,'russian',True)
#print(tokens[:10])

text = nltk.Text(tokens)

#http://www.nltk.org/_modules/nltk/collocations.html
finder_bi = BigramCollocationFinder.from_words(text)
finder_thr = TrigramCollocationFinder.from_words(text)

print(finder_bi.nbest(bigram_measures.pmi, 10))
print(finder_thr.nbest(trigram_measures.pmi, 10))
```
На защите лабораторной продемонстрировать работу алгоритмов (собственный, nltk), показать список из top-30 3-грамм, объяснить различия в результатах.
